{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c8dbe4-dfb4-4649-ac9b-08f4334f64e6",
   "metadata": {},
   "source": [
    "# LeNet Computer Vision Model\n",
    "\n",
    "> https://doi.org/10.1109/5.726791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88162614",
   "metadata": {},
   "source": [
    "## Import modules and set random seed\n",
    "\n",
    "Seed is set for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c8cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:20:33.585142: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-24 14:20:33.586667: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 14:20:33.616502: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 14:20:33.617406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 14:20:34.213814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, List\n",
    "\n",
    "import numpy\n",
    "import tensorflow\n",
    "\n",
    "from keras import layers, losses\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.models import Sequential\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "numpy.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d831e9",
   "metadata": {},
   "source": [
    "## Download and preparte MNIST dataset\n",
    "\n",
    "1. As the `LeNet` model expects images to be of size *32 x 32*, all images within the *MNIST* dataset need to be scaled from *28 x 28* to *32 x 32*\n",
    "2. As *MNIST* images are in grayscale, we want to binarize them between 0 and 1 (white or black) by dividing their color value by 255\n",
    "3. As *MNIST* images are in grayscale, they do not have the color channel value that is expected by *Keras* `Conv2d` module. In other words, the *MNIST* dataset tensor structure only contains [`batchSize`, `height`, `width`] Thus, we need to add in a fourth dimension to make our tenors look like [`batchSize`, `height`, `width`, `channel`] where `channel` == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22334947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:20:35.131147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 14:20:35.131394: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "imagePadding: List[List[int]] = [[0, 0], [2, 2], [2, 2]]\n",
    "\n",
    "mnist: tuple[tuple[Any, Any]] = load_data()\n",
    "\n",
    "xTrain: ndarray = mnist[0][0]\n",
    "yTrain: ndarray = mnist[0][1]\n",
    "xTest: ndarray = mnist[1][0]\n",
    "yTest: ndarray = mnist[1][1]\n",
    "\n",
    "xTrain = tensorflow.pad(tensor=xTrain, paddings=imagePadding) / 255\n",
    "xTest = tensorflow.pad(tensor=xTest, paddings=imagePadding) / 255\n",
    "\n",
    "xTrain = tensorflow.expand_dims(input=xTrain, axis=3)\n",
    "xTest = tensorflow.expand_dims(input=xTest, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dd8d2",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "### Architecture\n",
    "\n",
    "[![https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
    "\n",
    "> Image from https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbecb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (60000, 28, 28, 6)        156       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (60000, 14, 14, 6)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " activation (Activation)     (60000, 14, 14, 6)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (60000, 10, 10, 16)       2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (60000, 5, 5, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (60000, 5, 5, 16)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (60000, 1, 1, 120)        48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (60000, 120)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (60000, 84)               10164     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (60000, 10)               850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet: Sequential = Sequential(name=\"LeNet\")\n",
    "lenet.add(layer=layers.Conv2D(filters=6, kernel_size=5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(pool_size=2))\n",
    "lenet.add(layer=layers.Activation(activation=\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(16, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(2))\n",
    "lenet.add(layers.Activation(\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(120, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.Flatten())\n",
    "lenet.add(layers.Dense(84, activation=\"tanh\"))\n",
    "lenet.add(layers.Dense(10, activation=\"softmax\"))\n",
    "lenet.build(input_shape=xTrain.shape)\n",
    "lenet.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e0136",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a4ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "797/797 [==============================] - 6s 6ms/step - loss: 1.4358 - accuracy: 0.4855 - val_loss: 0.4136 - val_accuracy: 0.8719\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.3827 - accuracy: 0.8822 - val_loss: 0.2615 - val_accuracy: 0.9194\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2886 - accuracy: 0.9086 - val_loss: 0.2251 - val_accuracy: 0.9319\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2334 - accuracy: 0.9269 - val_loss: 0.1786 - val_accuracy: 0.9439\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2044 - accuracy: 0.9362 - val_loss: 0.1653 - val_accuracy: 0.9489\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1815 - accuracy: 0.9421 - val_loss: 0.1458 - val_accuracy: 0.9529\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1623 - accuracy: 0.9482 - val_loss: 0.1288 - val_accuracy: 0.9609\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1519 - accuracy: 0.9522 - val_loss: 0.1836 - val_accuracy: 0.9430\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1367 - accuracy: 0.9565 - val_loss: 0.1126 - val_accuracy: 0.9650\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1269 - accuracy: 0.9600 - val_loss: 0.1147 - val_accuracy: 0.9651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffac2d8b1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFolder: Path = Path(\"logs/lenet-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback: TensorBoard = TensorBoard(\n",
    "    log_dir=logFolder,\n",
    "    histogram_freq=1,\n",
    "    write_images=True,\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x=xTrain,\n",
    "    y=yTrain,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904d78a2",
   "metadata": {},
   "source": [
    "## Evaluate the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a582ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11743690073490143, 0.9616000056266785]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(\n",
    "    x=xTest,\n",
    "    y=yTest,\n",
    "    batch_size=64,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3f71567",
   "metadata": {},
   "source": [
    "## Save the model to disk\n",
    "\n",
    "> The model is saved in `SavedModel` and `TFLite` formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4ca5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/nsynovic/documents/projects/personal/dl-examples/dl_examples/models/lenet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/nsynovic/documents/projects/personal/dl-examples/dl_examples/models/lenet/assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt6an64xb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt6an64xb/assets\n",
      "2023-04-24 14:21:26.641241: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-04-24 14:21:26.641276: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-04-24 14:21:26.641863: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpt6an64xb\n",
      "2023-04-24 14:21:26.643079: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-24 14:21:26.643090: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpt6an64xb\n",
      "2023-04-24 14:21:26.647072: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-04-24 14:21:26.648075: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-04-24 14:21:26.687819: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpt6an64xb\n",
      "2023-04-24 14:21:26.696775: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 54915 microseconds.\n",
      "2023-04-24 14:21:26.724785: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "pbPath: Path = Path(\"models/lenet\").resolve()\n",
    "tfLitePath: Path = Path(\"models/lenet.tflite\").resolve()\n",
    "\n",
    "lenet.save(\n",
    "    filepath=pbPath,\n",
    "    overwrite=True,\n",
    "    save_format=\"tf\",\n",
    ")\n",
    "\n",
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model=lenet)\n",
    "tfLite = converter.convert()\n",
    "\n",
    "with open(tfLitePath, \"wb\") as model:\n",
    "    model.write(tfLite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
