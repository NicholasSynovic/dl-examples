{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c8dbe4-dfb4-4649-ac9b-08f4334f64e6",
   "metadata": {},
   "source": [
    "# LeNet Computer Vision Model\n",
    "\n",
    "> https://doi.org/10.1109/5.726791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88162614",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.models import Sequential\n",
    "from numpy import ndarray\n",
    "from typing import Any, List\n",
    "from keras import layers, losses\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d831e9",
   "metadata": {},
   "source": [
    "## Download and preparte MNIST dataset\n",
    "\n",
    "1. As the `LeNet` model expects images to be of size *32 x 32*, all images within the *MNIST* dataset need to be scaled from *28 x 28* to *32 x 32*\n",
    "2. As *MNIST* images are in grayscale, we want to binarize them between 0 and 1 (white or black) by dividing their color value by 255\n",
    "3. As *MNIST* images are in grayscale, they do not have the color channel value that is expected by *Keras* `Conv2d` module. In other words, the *MNIST* dataset tensor structure only contains [`batchSize`, `height`, `width`] Thus, we need to add in a fourth dimension to make our tenors look like [`batchSize`, `height`, `width`, `channel`] where `channel` == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22334947",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePadding: List[List[int]] = [[0, 0], [2, 2], [2, 2]]\n",
    "\n",
    "mnist: tuple[tuple[Any, Any]] = load_data()\n",
    "\n",
    "xTrain: ndarray = mnist[0][0]\n",
    "yTrain: ndarray = mnist[0][1]\n",
    "xTest: ndarray = mnist[1][0]\n",
    "yTest: ndarray = mnist[1][1]\n",
    "\n",
    "xTrain = tensorflow.pad(tensor=xTrain, paddings=imagePadding) / 255\n",
    "xTest = tensorflow.pad(tensor=xTest, paddings=imagePadding) / 255\n",
    "\n",
    "xTrain = tensorflow.expand_dims(input=xTrain, axis=3)\n",
    "xTest = tensorflow.expand_dims(input=xTest, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dd8d2",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "### Architecture\n",
    "\n",
    "[![https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
    "\n",
    "> Image from https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet: Sequential = Sequential(name=\"LeNet\")\n",
    "lenet.add(layer=layers.Conv2D(filters=6, kernel_size=5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(pool_size=2))\n",
    "lenet.add(layer=layers.Activation(activation=\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(16, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(2))\n",
    "lenet.add(layers.Activation(\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(120, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.Flatten())\n",
    "lenet.add(layers.Dense(84, activation=\"tanh\"))\n",
    "lenet.add(layers.Dense(10, activation=\"softmax\"))\n",
    "lenet.build(input_shape=xTrain.shape)\n",
    "lenet.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e0136",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a4ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1383 - accuracy: 0.9559 - val_loss: 0.1118 - val_accuracy: 0.9636\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 12s 16ms/step - loss: 0.1283 - accuracy: 0.9590 - val_loss: 0.1209 - val_accuracy: 0.9627\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1206 - accuracy: 0.9612 - val_loss: 0.0981 - val_accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1169 - accuracy: 0.9624 - val_loss: 0.0990 - val_accuracy: 0.9693\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1093 - accuracy: 0.9658 - val_loss: 0.1156 - val_accuracy: 0.9652\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1045 - accuracy: 0.9670 - val_loss: 0.1190 - val_accuracy: 0.9610\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.1009 - accuracy: 0.9676 - val_loss: 0.1109 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.0978 - accuracy: 0.9690 - val_loss: 0.0804 - val_accuracy: 0.9753\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.0941 - accuracy: 0.9704 - val_loss: 0.0953 - val_accuracy: 0.9716\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 13s 16ms/step - loss: 0.0927 - accuracy: 0.9701 - val_loss: 0.0878 - val_accuracy: 0.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabad4334c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFolder: Path = Path(\"logs/lenet-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback: TensorBoard = TensorBoard(\n",
    "    log_dir=logFolder,\n",
    "    histogram_freq=1,\n",
    "    write_images=True,\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x=xTrain,\n",
    "    y=yTrain,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    validation_split=0.15\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904d78a2",
   "metadata": {},
   "source": [
    "## Evaluate the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a582ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0963 - accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09631393104791641, 0.9692999720573425]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(\n",
    "    x=xTest,\n",
    "    y=yTest,\n",
    "    batch_size=64,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
