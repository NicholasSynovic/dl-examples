{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c8dbe4-dfb4-4649-ac9b-08f4334f64e6",
   "metadata": {},
   "source": [
    "# LeNet Computer Vision Model\n",
    "\n",
    "> https://doi.org/10.1109/5.726791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88162614",
   "metadata": {},
   "source": [
    "## Import modules and set random seed\n",
    "\n",
    "Seed is set for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c8cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 13:31:55.088331: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-24 13:31:55.090185: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 13:31:55.120311: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-24 13:31:55.121692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 13:31:55.796626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.models import Sequential\n",
    "from numpy import ndarray\n",
    "from typing import Any, List\n",
    "from keras import layers, losses\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d831e9",
   "metadata": {},
   "source": [
    "## Download and preparte MNIST dataset\n",
    "\n",
    "1. As the `LeNet` model expects images to be of size *32 x 32*, all images within the *MNIST* dataset need to be scaled from *28 x 28* to *32 x 32*\n",
    "2. As *MNIST* images are in grayscale, we want to binarize them between 0 and 1 (white or black) by dividing their color value by 255\n",
    "3. As *MNIST* images are in grayscale, they do not have the color channel value that is expected by *Keras* `Conv2d` module. In other words, the *MNIST* dataset tensor structure only contains [`batchSize`, `height`, `width`] Thus, we need to add in a fourth dimension to make our tenors look like [`batchSize`, `height`, `width`, `channel`] where `channel` == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22334947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 13:31:56.658772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 13:31:56.659012: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "imagePadding: List[List[int]] = [[0, 0], [2, 2], [2, 2]]\n",
    "\n",
    "mnist: tuple[tuple[Any, Any]] = load_data()\n",
    "\n",
    "xTrain: ndarray = mnist[0][0]\n",
    "yTrain: ndarray = mnist[0][1]\n",
    "xTest: ndarray = mnist[1][0]\n",
    "yTest: ndarray = mnist[1][1]\n",
    "\n",
    "xTrain = tensorflow.pad(tensor=xTrain, paddings=imagePadding) / 255\n",
    "xTest = tensorflow.pad(tensor=xTest, paddings=imagePadding) / 255\n",
    "\n",
    "xTrain = tensorflow.expand_dims(input=xTrain, axis=3)\n",
    "xTest = tensorflow.expand_dims(input=xTest, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dd8d2",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "### Architecture\n",
    "\n",
    "[![https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
    "\n",
    "> Image from https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbecb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (60000, 28, 28, 6)        156       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (60000, 14, 14, 6)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " activation (Activation)     (60000, 14, 14, 6)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (60000, 10, 10, 16)       2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (60000, 5, 5, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (60000, 5, 5, 16)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (60000, 1, 1, 120)        48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (60000, 120)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (60000, 84)               10164     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (60000, 10)               850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet: Sequential = Sequential(name=\"LeNet\")\n",
    "lenet.add(layer=layers.Conv2D(filters=6, kernel_size=5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(pool_size=2))\n",
    "lenet.add(layer=layers.Activation(activation=\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(16, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.AveragePooling2D(2))\n",
    "lenet.add(layers.Activation(\"sigmoid\"))\n",
    "lenet.add(layers.Conv2D(120, 5, activation=\"tanh\"))\n",
    "lenet.add(layers.Flatten())\n",
    "lenet.add(layers.Dense(84, activation=\"tanh\"))\n",
    "lenet.add(layers.Dense(10, activation=\"softmax\"))\n",
    "lenet.build(input_shape=xTrain.shape)\n",
    "lenet.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e0136",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a4ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "797/797 [==============================] - 6s 6ms/step - loss: 1.4594 - accuracy: 0.4700 - val_loss: 0.4139 - val_accuracy: 0.8691\n",
      "Epoch 2/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.3919 - accuracy: 0.8753 - val_loss: 0.3014 - val_accuracy: 0.9026\n",
      "Epoch 3/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.3025 - accuracy: 0.9039 - val_loss: 0.2254 - val_accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2558 - accuracy: 0.9192 - val_loss: 0.2078 - val_accuracy: 0.9361\n",
      "Epoch 5/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2277 - accuracy: 0.9278 - val_loss: 0.2028 - val_accuracy: 0.9368\n",
      "Epoch 6/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.2076 - accuracy: 0.9331 - val_loss: 0.1607 - val_accuracy: 0.9514\n",
      "Epoch 7/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1865 - accuracy: 0.9401 - val_loss: 0.1485 - val_accuracy: 0.9517\n",
      "Epoch 8/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1730 - accuracy: 0.9445 - val_loss: 0.1342 - val_accuracy: 0.9560\n",
      "Epoch 9/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1619 - accuracy: 0.9481 - val_loss: 0.1378 - val_accuracy: 0.9573\n",
      "Epoch 10/10\n",
      "797/797 [==============================] - 5s 6ms/step - loss: 0.1532 - accuracy: 0.9508 - val_loss: 0.1245 - val_accuracy: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febe3ac3310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFolder: Path = Path(\"logs/lenet-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "tensorboard_callback: TensorBoard = TensorBoard(\n",
    "    log_dir=logFolder,\n",
    "    histogram_freq=1,\n",
    "    write_images=True,\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x=xTrain,\n",
    "    y=yTrain,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    validation_split=0.15\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904d78a2",
   "metadata": {},
   "source": [
    "## Evaluate the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a582ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 0.1321 - accuracy: 0.9576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13214370608329773, 0.9575999975204468]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(\n",
    "    x=xTest,\n",
    "    y=yTest,\n",
    "    batch_size=64,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
